service: improve-v5

plugins:
  - serverless-pseudo-parameters

custom:
  resource-identifier: resources
  jobQueueName: '${self:service}-${self:custom.resource-identifier}-job-queue-${opt:stage, self:provider.stage}'
  jobDefinitionName: '${self:service}-${self:custom.resource-identifier}-job-definition-${opt:stage, self:provider.stage}'

provider:
  name: aws
  region: us-west-2
  runtime: nodejs12.x
  timeout: 900
  memorySize: 192
  environment:
    #
    # Customize your deployment here:
    #
    JOIN_REWARDS_JOB_ARRAY_SIZE: 3 # Number of instances to launch in the compute environment
    TRAINING_JOB_SCHEDULE: rate(24 hours) # trades off between learning latency and cost
    REWARD_ASSIGNMENT_JOB_SCHEDULE: rate(4 hours) # trades off between learning latency and cost
    FEATURE_TRAINING_IMAGE: 117097735164.dkr.ecr.us-west-2.amazonaws.com/improve_trainer-dev:latest 
    FEATURE_TRAINING_INSTANCE_TYPE: ml.m5.large
    FEATURE_TRAINING_INSTANCE_COUNT: 1
    FEATURE_TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    XGBOOST_TRAINING_IMAGE: 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3 # XGBoost (0.90) do this after validating everything # 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest  #
    XGBOOST_TRAINING_INSTANCE_TYPE: ml.m5.12xlarge
    XGBOOST_TRAINING_INSTANCE_COUNT: 4
    XGBOOST_TRAINING_VOLUME_SIZE_IN_GB: 10
    XGBOOST_TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    DEFAULT_REWARD_WINDOW_IN_SECONDS: 7200 # 2 hours
    FIREHOSE_WORKER_MEMORY_IN_MB: 1024 # must be large enough to unpack the entire firehose buffer into memory. Buffer size is configured in resources/serverless.yml
    REWARD_ASSIGNMENT_WORKER_MEMORY_IN_MB: 1024 # must be large enough to hold a reward window's worth of events for a single history shard
    REWARD_ASSIGNMENT_WORKER_MAX_PAYLOAD_IN_MB: 32 # If the size of the compressed data processed for a reward window exceeds this size, a reshard will be triggered, splitting the shard in half
    REWARD_ASSIGNMENT_WORKER_COUNT: 1 # larger deployments with many shards will want to increase this number. Processes the least recently processed shards each time a firehose unpack finishes. S3 and lambda costs will increase approximately linearly.
    # 
    # End customization
    #
    FEATURE_TRAINING_ROLE_ARN: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.SagemakerExecutionRoleArn}
    FIREHOSE_DELIVERY_STREAM_NAME: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.V4Firehose}
    RECORDS_BUCKET: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
    MODELS_BUCKET: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.ModelsS3Bucket}
    EFS_FILE_PATH: /mnt/efs
    SERVICE: ${self:service}
    STAGE: ${opt:stage, self:provider.stage}
  httpApi: # FIX MOVE DOWN?
    cors: true
  iamRoleStatements: 
    - Effect: Allow
      Action:
        - "firehose:*"
      Resource: "arn:aws:firehose:*:*:deliverystream/${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.V4Firehose}"
    - Effect: Allow
      Action:
        - "s3:*"
      Resource: 'arn:aws:s3:::${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-*'
    - Effect: Allow
      Action:
        - "sagemaker:*"
      Resource: '*'
    - Effect: Allow
      Action:
        - "iam:PassRole"
      Resource: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.SagemakerExecutionRoleArn}
    - Effect: Allow
      Action:
        - "lambda:InvokeFunction"
        - "lambda:InvokeAsync"
      Resource: "arn:aws:lambda:${self:provider.region}:*:function:${self:service}-${opt:stage, self:provider.stage}-*"
    - Effect: Allow
      Action:
        - logs:CreateLogGroup
        - logs:CreateLogStream
        - logs:PutLogEvents
        - ec2:CreateNetworkInterface
        - ec2:DescribeNetworkInterfaces
        - ec2:DeleteNetworkInterface
      Resource: "*"
    - Effect: Allow
      Action:
        - elasticfilesystem:ClientMount
        - elasticfilesystem:ClientRootAccess
        - elasticfilesystem:ClientWrite
        - elasticfilesystem:DescribeMountTargets
      Resource: "*"
    - Effect: Allow
      Action:
        - batch:SubmitJob
      Resource: "arn:aws:batch:*:*:*"

package:
  individually: true
  exclude:
    - ./**
  include:
    - 'node_modules/**'
    
functions:
  track:
    description: Event Tracker HTTP API
    handler: src/track/http_api.track
    timeout: 6
    events:
      - httpApi:
          method: POST
          path: /track
    package:
      include:
        - 'src/track/**'
  unpackFirehose:
    description: Ingest Records from S3 to EFS
    handler: firehose.unpackFirehose
    memorySize: ${self:provider.environment.FIREHOSE_WORKER_MEMORY_IN_MB} 
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.FirehoseS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
    fileSystemConfig:
      localMountPath: /mnt/efs
      arn: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.AccessPointResourceArn}
    vpc:
      securityGroupIds: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.SecurityGroup}
      subnetIds: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.Subnet}
    package:
      include:
        - 'src/track/**'
  dispatchRewardAssignmentJobs:
    description: Dispatch Reward Assignment Batch Jobs
    runtime: python3.8
    handler: src/assign_rewards/start_job_lambda.lambda_handler
    events:
      - schedule: ${self:provider.environment.REWARD_ASSIGNMENT_JOB_SCHEDULE}
    environment:
      JOB_QUEUE_NAME: ${self:custom.jobQueueName}
      JOB_DEFINITION_NAME: ${self:custom.jobDefinitionName}
    package:
      include:
        - 'src/assign_rewards/start_job_lambda.py'
  dispatchTrainingJobs:
    description: Dispatch SageMaker Training Jobs
    handler: train.dispatchTrainingJobs
    events:
      - schedule: ${self:provider.environment.TRAINING_JOB_SCHEDULE}
    package:
      include:
        - 'src/train/**'
  featureModelCreated:
    description: Dispatch SageMaker Training Jobs
    handler: train.featureModelCreated
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: feature_models/
            - suffix: model.tar.gz
  xgboostModelCreated:
    handler: train.xgboostModelCreated
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: xgboost_models/
            - suffix: model.tar.gz
  unpackModels:
    handler: unpack_models.unpack
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: transformed_models/
            - suffix: model.tar.gz.out