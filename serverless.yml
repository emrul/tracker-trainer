service: improve-v5

plugins:
  - serverless-pseudo-parameters

custom:
  resource-identifier: resources
  jobQueueName: '${self:service}-${self:custom.resource-identifier}-job-queue-${opt:stage, self:provider.stage}'
  jobDefinitionName: '${self:service}-${self:custom.resource-identifier}-job-definition-${opt:stage, self:provider.stage}'

provider:
  name: aws
  region: us-west-2
  runtime: nodejs12.x
  timeout: 900
  memorySize: 192
  environment:
    #
    # Customize your deployment here:
    #
    JOIN_REWARDS_JOB_ARRAY_SIZE: 3 # Number of instances to launch in the compute environment
    TRAINING_JOB_SCHEDULE: rate(24 hours) # trades off between learning latency and cost
    FEATURE_TRAINING_IMAGE: 117097735164.dkr.ecr.us-west-2.amazonaws.com/improve_trainer-dev:latest 
    FEATURE_TRAINING_INSTANCE_TYPE: ml.m5.large
    FEATURE_TRAINING_INSTANCE_COUNT: 1
    FEATURE_TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    TRANSFORM_INSTANCE_TYPE: ml.m5.large
    TRANSFORM_INSTANCE_COUNT: 4
    XGBOOST_TRAINING_IMAGE: 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3 # XGBoost (0.90) do this after validating everything # 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest  #
    XGBOOST_TRAINING_INSTANCE_TYPE: ml.m5.12xlarge
    XGBOOST_TRAINING_INSTANCE_COUNT: 4
    XGBOOST_TRAINING_VOLUME_SIZE_IN_GB: 10
    XGBOOST_TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    DEFAULT_REWARD_WINDOW_IN_SECONDS: 7200 # 2 hours
    FIREHOSE_WORKER_MEMORY_IN_MB: 1024 # must be large enough to unpack the entire firehose buffer into memory. Buffer size is configured in resources/serverless.yml
    REWARD_ASSIGNMENT_WORKER_MEMORY_IN_MB: 1024 # must be large enough to hold a reward window's worth of events for a single history shard
    REWARD_ASSIGNMENT_WORKER_MAX_PAYLOAD_IN_MB: 32 # If the size of the compressed data processed for a reward window exceeds this size, a reshard will be triggered, splitting the shard in half
    REWARD_ASSIGNMENT_WORKER_COUNT: 1 # larger deployments with many shards will want to increase this number. Processes the least recently processed shards each time a firehose unpack finishes. S3 and lambda costs will increase approximately linearly.
    #
    # When configured with a low number of workers, increasing this may further decrease Lambda and S3 costs by re-processing history shards less often.
    REWARD_ASSIGNMENT_REPROCESS_SHARD_WAIT_TIME_IN_SECONDS: 30 # 900 # 15 minutes.  Don't re-process the same shard any more frequently than this number, regardless of the number of workers.
    RESHARD_WORKER_MEMORY_IN_MB: 1024 # must be large enough to unpack a single day's data for one shard in memory.
    RESHARD_WORKER_RESERVED_CONCURRENCY: 50 # limits maximum concurrency, but reduces the shared lambda pool for this AWS account in this region
    # 
    # End customization
    #
    VALIDATION_PROPORTION: .3 # CAUTION: changes require re-mapping all files in /rewarded_actions to the proper train/validation directories.
    FEATURE_TRAINING_ROLE_ARN: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.SagemakerExecutionRoleArn}
    FIREHOSE_DELIVERY_STREAM_NAME: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.V4Firehose}
    RECORDS_BUCKET: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
    MODELS_BUCKET: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.ModelsS3Bucket}
    EFS_FILE_PATH: /mnt/efs
    SERVICE: ${self:service}
    STAGE: ${opt:stage, self:provider.stage}
    PRE_DEPLOY_CHECK: ${file(predeploy.js)} # execute the pre deploy checks
  httpApi:
    cors: true
  iamRoleStatements: 
    - Effect: Allow
      Action:
        - "firehose:*"
      Resource: "arn:aws:firehose:*:*:deliverystream/${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.V4Firehose}"
    - Effect: Allow
      Action:
        - "s3:*"
      Resource: 'arn:aws:s3:::${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-*'
    - Effect: Allow
      Action:
        - "sagemaker:*"
      Resource: '*'
    - Effect: Allow
      Action:
        - "iam:PassRole"
      Resource: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.SagemakerExecutionRoleArn}
    - Effect: Allow
      Action:
        - "lambda:InvokeFunction"
        - "lambda:InvokeAsync"
      Resource: "arn:aws:lambda:${self:provider.region}:*:function:${self:service}-${opt:stage, self:provider.stage}-*"
    - Effect: Allow
      Action:
        - logs:CreateLogGroup
        - logs:CreateLogStream
        - logs:PutLogEvents
        - ec2:CreateNetworkInterface
        - ec2:DescribeNetworkInterfaces
        - ec2:DeleteNetworkInterface
      Resource: "*"
    - Effect: Allow
      Action:
        - elasticfilesystem:ClientMount
        - elasticfilesystem:ClientRootAccess
        - elasticfilesystem:ClientWrite
        - elasticfilesystem:DescribeMountTargets
      Resource: "*"
    - Effect: Allow
      Action:
        - batch:SubmitJob
      Resource: "arn:aws:batch:*:*:*"

functions:
  joinRewards:
    runtime: python3.8
    handler: src/joinRewards.lambda_handler
    description: Launch AWS Batch jobs
    environment:
      JOB_QUEUE_NAME: ${self:custom.jobQueueName}
      JOB_DEFINITION_NAME: ${self:custom.jobDefinitionName}
  track:
    handler: http_api.track
    timeout: 6
    events:
      - httpApi:
          method: POST
          path: /track
  dispatchTrainingJobs:
    handler: train.dispatchTrainingJobs
    events:
      - schedule: ${self:provider.environment.TRAINING_JOB_SCHEDULE}
  unpackFirehose:
    handler: firehose.unpackFirehose
    memorySize: ${self:provider.environment.FIREHOSE_WORKER_MEMORY_IN_MB} 
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.FirehoseS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
    fileSystemConfig:
      localMountPath: /mnt/efs
      arn: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.AccessPointResourceArn}
    vpc:
      securityGroupIds: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.SecurityGroup}
      subnetIds: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.Subnet}
  dispatchRewardAssignmentWorkers:
    handler: history.dispatchRewardAssignmentWorkers
    reservedConcurrency: 1 # enforce serial execution to avoid potential concurrency issues
  assignRewards:
    handler: history.assignRewards
    memorySize: ${self:provider.environment.REWARD_ASSIGNMENT_WORKER_MEMORY_IN_MB}
  splitShard:
    handler: shard.splitShard
  splitFile:
    handler: shard.splitFile
    memorySize: ${self:provider.environment.RESHARD_WORKER_MEMORY_IN_MB}
    reservedConcurrency: ${self:provider.environment.RESHARD_WORKER_RESERVED_CONCURRENCY}
  featureModelCreated:
    handler: train.featureModelCreated
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: feature_models/
            - suffix: model.tar.gz
  xgboostModelCreated:
    handler: train.xgboostModelCreated
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: xgboost_models/
            - suffix: model.tar.gz
  transformJobCompleted:
    handler: train.transformJobCompleted
    events:
      - cloudwatchEvent:
          event:
            source:
              - 'aws.sagemaker'
            detail-type:
              - 'SageMaker Transform Job State Change'
            detail:
              TransformJobStatus:
                - Completed
  unpackModels:
    handler: unpack_models.unpack
    events:
      - s3:
          bucket: ${cf:${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: transformed_models/
            - suffix: model.tar.gz.out