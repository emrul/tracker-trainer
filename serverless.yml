service: improve-v5

plugins:
  - serverless-pseudo-parameters

custom:
  resource-identifier: resources

provider:
  name: aws
  region: us-west-2
  runtime: nodejs12.x
  timeout: 900
  memorySize: 192
  environment:
    #
    # Customize your deployment here:
    #
    TRAINING_JOB_SCHEDULE: rate(24 hours) # trades off between learning latency and cost
    FEATURE_TRAINING_IMAGE: 117097735164.dkr.ecr.us-west-2.amazonaws.com/improve_trainer-dev:latest 
    FEATURE_TRAINING_INSTANCE_TYPE: ml.m5.large
    FEATURE_TRAINING_INSTANCE_COUNT: 1
    FEATURE_TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    TRANSFORM_INSTANCE_TYPE: ml.m5.large
    TRANSFORM_INSTANCE_COUNT: 4
    XGBOOST_TRAINING_IMAGE: 246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:0.90-2-cpu-py3 # XGBoost (0.90) do this after validating everything # 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest  #
    XGBOOST_TRAINING_INSTANCE_TYPE: ml.m5.xlarge
    XGBOOST_TRAINING_INSTANCE_COUNT: 4
    XGBOOST_TRAINING_VOLUME_SIZE_IN_GB: 10
    XGBOOST_TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    DEFAULT_REWARD_WINDOW_IN_SECONDS: 7200 # 2 hours
    FIREHOSE_WORKER_MEMORY_IN_MB: 1024 # must be large enough to unpack the entire firehose buffer into memory. Buffer size is configured in resources/serverless.yml
    REWARD_ASSIGNMENT_WORKER_MEMORY_IN_MB: 1024 # must be large enough to hold a reward window's worth of events for a single history shard
    REWARD_ASSIGNMENT_WORKER_MAX_PAYLOAD_IN_MB: 32 # If the size of the compressed data processed for a reward window exceeds this size, a reshard will be triggered, splitting the shard in half
    REWARD_ASSIGNMENT_WORKER_COUNT: 1 # larger deployments with many shards will want to increase this number. Processes the least recently processed shards each time a firehose unpack finishes. S3 and lambda costs will increase approximately linearly.
    #
    # When configured with a low number of workers, increasing this may further decrease Lambda and S3 costs by re-processing history shards less often.
    REWARD_ASSIGNMENT_REPROCESS_SHARD_WAIT_TIME_IN_SECONDS: 30 # 900 # 15 minutes.  Don't re-process the same shard any more frequently than this number, regardless of the number of workers.
    RESHARD_WORKER_MEMORY_IN_MB: 1024 # must be large enough to unpack a single day's data for one shard in memory.
    RESHARD_WORKER_RESERVED_CONCURRENCY: 50 # limits maximum concurrency, but reduces the shared lambda pool for this AWS account in this region
    # 
    # End customization
    #
    VALIDATION_PROPORTION: .3 # CAUTION: changes require re-mapping all files in /rewarded_actions to the proper train/validation directories.
    #FEATURE_TRAINING_ROLE_ARN: ${cf:${self:service}-resources-${opt:stage, self:provider.stage}.SagemakerExecutionRoleArn}
    #FIREHOSE_DELIVERY_STREAM_NAME: ${cf:${self:service}-resources-${opt:stage, self:provider.stage}.V4Firehose}
    #RECORDS_BUCKET: ${cf:${self:service}-resources-${opt:stage, self:provider.stage}.RecordsS3Bucket}
    #MODELS_BUCKET: ${cf:${self:service}-resources-${opt:stage, self:provider.stage}.ModelsS3Bucket}
    SERVICE: ${self:service}
    STAGE: ${opt:stage, self:provider.stage}
    PRE_DEPLOY_CHECK: ${file(predeploy.js)} # execute the pre deploy checks
  iamRoleStatements: 
    - Effect: Allow
      Action:
        - "firehose:*"
      Resource: !GetAtt V4Firehose.Arn
    - Effect: Allow
      Action:
        - "s3:*"
      Resource: 'arn:aws:s3:::${self:service}-resources-${opt:stage, self:provider.stage}-*'
    - Effect: Allow
      Action:
        - "sagemaker:*"
      Resource: '*'
    - Effect: Allow
      Action:
        - "iam:PassRole"
      Resource: !GetAtt SagemakerExecutionRole.Arn
      #${cf:${self:service}-resources-${opt:stage, self:provider.stage}.SagemakerExecutionRoleArn}
    - Effect: Allow
      Action:
        - "lambda:InvokeFunction"
        - "lambda:InvokeAsync"
      Resource: "arn:aws:lambda:${self:provider.region}:*:function:${self:service}-${opt:stage, self:provider.stage}-*"
    - Effect: Allow
      Action:
        - logs:CreateLogGroup
        - logs:CreateLogStream
        - logs:PutLogEvents
        - ec2:CreateNetworkInterface
        - ec2:DescribeNetworkInterfaces
        - ec2:DeleteNetworkInterface
      Resource: "*"
    - Effect: Allow
      Action:
        - elasticfilesystem:ClientMount
        - elasticfilesystem:ClientRootAccess
        - elasticfilesystem:ClientWrite
        - elasticfilesystem:DescribeMountTargets
      Resource: "*"

functions:
  joinRewards:
    runtime: python3.6
    timeout: 6
    events:
    - http:
        method: get
        path: greet
        cors: true
    handler: src/handler.hello
  track:
    handler: http_api.track
    timeout: 6
    events:
      - http:
          path: track
          method: post
          cors: true
        #  private: true # uncomment if you wish to require authentication via API Gateway
  dispatchTrainingJobs:
    handler: train.dispatchTrainingJobs
    events:
      - schedule: ${self:provider.environment.TRAINING_JOB_SCHEDULE}
  unpackFirehose:
    handler: firehose.unpackFirehose
    memorySize: ${self:provider.environment.FIREHOSE_WORKER_MEMORY_IN_MB} 
    events:
      - s3:
          bucket:
            Ref: FirehoseS3Bucket
          #${cf:${self:service}-resources-${opt:stage, self:provider.stage}.FirehoseS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
    fileSystemConfig:
      localMountPath: /mnt/efs
      arn: 
        Fn::GetAtt: [ AccessPointResource, Arn ]
    vpc:
      securityGroupIds:
        - Ref: SecurityGroup
      subnetIds:
        - Ref: Subnet
  dispatchRewardAssignmentWorkers:
    handler: history.dispatchRewardAssignmentWorkers
    reservedConcurrency: 1 # enforce serial execution to avoid potential concurrency issues
  assignRewards:
    handler: history.assignRewards
    memorySize: ${self:provider.environment.REWARD_ASSIGNMENT_WORKER_MEMORY_IN_MB}
  splitShard:
    handler: shard.splitShard
  splitFile:
    handler: shard.splitFile
    memorySize: ${self:provider.environment.RESHARD_WORKER_MEMORY_IN_MB}
    reservedConcurrency: ${self:provider.environment.RESHARD_WORKER_RESERVED_CONCURRENCY}
  featureModelCreated:
    handler: train.featureModelCreated
    events:
      - s3:
          bucket:
            Ref: RecordsS3Bucket 
          #${cf:${self:service}-resources-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: feature_models/
            - suffix: model.tar.gz
  xgboostModelCreated:
    handler: train.xgboostModelCreated
    events:
      - s3:
          bucket: 
            Ref: RecordsS3Bucket
          #${cf:${self:service}-resources-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: xgboost_models/
            - suffix: model.tar.gz
  transformJobCompleted:
    handler: train.transformJobCompleted
    events:
      - cloudwatchEvent:
          event:
            source:
              - 'aws.sagemaker'
            detail-type:
              - 'SageMaker Transform Job State Change'
            detail:
              TransformJobStatus:
                - Completed
  unpackModels:
    handler: unpack_models.unpack
    events:
      - s3:
          bucket:
            Ref: RecordsS3Bucket 
          #${cf:${self:service}-resources-${opt:stage, self:provider.stage}.RecordsS3Bucket}
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: transformed_models/
            - suffix: model.tar.gz.out
  # demoFunction:
  #   handler: handler.default
  #   runtime: nodejs12.x
  #   description: A Demo Lambda Function to check whether efs is working or not
  #   fileSystemConfig:
  #     localMountPath: /mnt/efs
  #     arn: 
  #       Fn::GetAtt: [ AccessPointResource, Arn ]
  #   vpc:
  #     securityGroupIds:
  #       - Ref: SecurityGroup
  #     subnetIds:
  #       - Ref: Subnet

resources:
  Resources:
      FirehoseToS3Role:
        Type: AWS::IAM::Role
        Properties:
          RoleName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-FirehoseToS3Role
          AssumeRolePolicyDocument:
            Statement:
            - Effect: Allow
              Principal:
                Service:
                - firehose.amazonaws.com
              Action:
              - sts:AssumeRole
          Policies:
          - PolicyName: FirehoseToS3Policy
            PolicyDocument:
              Statement:
                - Effect: Allow
                  Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:PutObject
                  Resource: '*'
      SagemakerExecutionRole:
        Type: AWS::IAM::Role
        Properties:
          RoleName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-SagemakerExecutionRole
          AssumeRolePolicyDocument:
            Statement:
            - Effect: Allow
              Principal:
                Service:
                - firehose.amazonaws.com
              Action:
              - sts:AssumeRole
            - Effect: Allow
              Principal:
                Service:
                - sagemaker.amazonaws.com
              Action:
              - sts:AssumeRole
          ManagedPolicyArns:
          - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
          Policies:
          - PolicyName: SagemakerExecutionPolicy
            PolicyDocument:
              Statement:
                - Effect: Allow
                  Action:
                  - s3:GetObject
                  - s3:DeleteObject
                  - s3:ListBucket
                  - s3:PutObject
                  Resource: "arn:aws:s3:::${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-*-#{AWS::AccountId}*"
      FirehoseS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-firehose-#{AWS::AccountId}
      V4Firehose:
        Type: AWS::KinesisFirehose::DeliveryStream
        Properties:
          DeliveryStreamName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-firehose
          S3DestinationConfiguration:
            BucketARN:
              Fn::Join:
              - ''
              - - 'arn:aws:s3:::'
                - Ref: FirehoseS3Bucket
            BufferingHints:
              IntervalInSeconds: 300
              SizeInMBs: 128
            CompressionFormat: "GZIP"
            RoleARN: { Fn::GetAtt: [ FirehoseToS3Role, Arn ] }
      RecordsS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-records-#{AWS::AccountId}
      ModelsS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-models-#{AWS::AccountId}
      PublicModelsS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${self:custom.resource-identifier}-${opt:stage, self:provider.stage}-public-models-#{AWS::AccountId}
      VPC:
        Type: AWS::EC2::VPC
        Properties:
          CidrBlock: 10.0.0.0/16
      InternetGateway:
        Type: AWS::EC2::InternetGateway
      RouteTable:
        Type: AWS::EC2::RouteTable
        Properties:
          VpcId:
            Ref: VPC
      VPCGatewayAttachment:
        Type: AWS::EC2::VPCGatewayAttachment
        Properties:
          VpcId:
            Ref: VPC
          InternetGatewayId:
            Ref: InternetGateway
      SecurityGroup:
        Type: AWS::EC2::SecurityGroup
        Properties:
          GroupDescription: EC2 Security Group for instances launched in the VPC by Batch
          VpcId:
            Ref: VPC
          SecurityGroupIngress:
            - IpProtocol: tcp
              FromPort: 2049 #Had to open a connection for lambda to communicate with AWS EFS.
              ToPort: 2049  #In AWS internally in a security group port 2049 is stored for network file transfer, 
              CidrIp: 0.0.0.0/0 #so in order to make the lambda to invoke successfully I need to add an in-bound policy or ingress policy which will allow access on that port 
      Subnet:
        Type: AWS::EC2::Subnet
        Properties:
          CidrBlock: 10.0.0.0/24
          VpcId:
            Ref: VPC
          MapPublicIpOnLaunch: 'True'
      Route:
        Type: AWS::EC2::Route
        Properties:
          RouteTableId:
            Ref: RouteTable
          DestinationCidrBlock: 0.0.0.0/0
          GatewayId:
            Ref: InternetGateway
      SubnetRouteTableAssociation:
        Type: AWS::EC2::SubnetRouteTableAssociation
        Properties:
          RouteTableId:
            Ref: RouteTable
          SubnetId:
            Ref: Subnet
      IamInstanceProfile:
        Type: AWS::IAM::InstanceProfile
        Properties:
          Roles:
          - Ref: EcsInstanceRole
      EcsInstanceRole:
        Type: AWS::IAM::Role
        Properties:
          AssumeRolePolicyDocument:
            Version: '2008-10-17'
            Statement:
                Effect: Allow
                Principal:
                  Service: 
                  - ec2.amazonaws.com
                Action: sts:AssumeRole
          ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
      BatchServiceRole:
        Type: AWS::IAM::Role
        Properties:
          AssumeRolePolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Principal:
                  Service: 
                  - batch.amazonaws.com
                Action: sts:AssumeRole
          ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole
      AmazonEC2SpotFleetRole:
        Type: AWS::IAM::Role
        Properties:
          AssumeRolePolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Principal:
                  Service: 
                  - spotfleet.amazonaws.com
                Action: sts:AssumeRole
          ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole
      ComputeEnvironment:
        Type: AWS::Batch::ComputeEnvironment
        Properties:
          Type: MANAGED
          ServiceRole: 
            Ref: BatchServiceRole
          ComputeEnvironmentName: '${self:service}-${self:custom.resource-identifier}-compute-environment-${opt:stage, self:provider.stage}'
          ComputeResources:
            MaxvCpus: 128
            SecurityGroupIds:
              - Ref: SecurityGroup
            Type: SPOT
            SpotIamFleetRole: 
              Ref: AmazonEC2SpotFleetRole
            # aws-ec2-spot-fleet-tagging-role
            Subnets:
              - Ref: Subnet
            MinvCpus: 0
            InstanceRole:  
              Ref: IamInstanceProfile
            InstanceTypes:
              - optimal
            Tags: {"Name": "Batch Instance - ${self:service}-${self:custom.resource-identifier}"}
            DesiredvCpus: 0
          State: ENABLED
      JobQueue:
        Type: AWS::Batch::JobQueue
        Properties:
          ComputeEnvironmentOrder:
            - Order: 1
              ComputeEnvironment: 
                Ref: ComputeEnvironment
          State: ENABLED
          Priority: 1
          JobQueueName: '${self:service}-${self:custom.resource-identifier}-job-queue-${opt:stage, self:provider.stage}'
      JobDefinition:
        Type: "AWS::Batch::JobDefinition"
        Properties:
          Type: Container
          ContainerProperties: 
            Command: 
              - ls
            Memory: 128
            Vcpus: 1
            Image: '#{AWS::AccountId}.dkr.ecr.#{AWS::Region}.amazonaws.com/batch-processing-job-repository-${opt:stage, self:provider.stage}:latest'
          JobDefinitionName: '${self:service}-${self:custom.resource-identifier}-job-definition-${opt:stage, self:provider.stage}'
          RetryStrategy: 
            Attempts: 1
      BatchProcessRepository: 
        Type: AWS::ECR::Repository
        Properties: 
          RepositoryName: "batch-processing-job-repository-${opt:stage, self:provider.stage}"
          RepositoryPolicyText: 
            Version: "2012-10-17"
            Statement: 
              - 
                Sid: AllowPushPull
                Effect: Allow
                Principal: 
                  AWS: 
                  - !Sub arn:aws:iam::${AWS::AccountId}:role/${EcsInstanceRole}
                Action: 
                  - "ecr:GetDownloadUrlForLayer"
                  - "ecr:BatchGetImage"
                  - "ecr:BatchCheckLayerAvailability"
                  - "ecr:PutImage"
                  - "ecr:InitiateLayerUpload"
                  - "ecr:UploadLayerPart"
                  - "ecr:CompleteLayerUpload"
      FileSystemResource:
        Type: 'AWS::EFS::FileSystem'
        Properties:
          PerformanceMode: maxIO
          Encrypted: true
          FileSystemTags:
            - Key: Name
              Value: "${self:service}-${self:custom.resource-identifier}-file-system-${opt:stage, self:provider.stage}"
          FileSystemPolicy:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action:
                  - "elasticfilesystem:ClientMount"
                Principal:
                  AWS: "*"
      MountTarget:
        Type: AWS::EFS::MountTarget
        Properties:
          FileSystemId: 
            Ref: FileSystemResource
          SecurityGroups:
            - Ref: SecurityGroup
          SubnetId: 
            Ref: Subnet
        DependsOn: Subnet
      AccessPointResource:
        Type: 'AWS::EFS::AccessPoint'
        Properties:
          FileSystemId: 
            Ref: FileSystemResource
          PosixUser:
            Uid: "1000"
            Gid: "1000"
          RootDirectory:
            CreationInfo:
              OwnerGid: "1000"
              OwnerUid: "1000"
              Permissions: "0777"
            Path: "/efs"
        DependsOn: MountTarget