custom:
  organization: acme # CHANGE ME BEFORE FIRST DEPLOYMENT
  project: demo # CHANGE ME BEFORE FIRST DEPLOYMENT

provider:
  environment:
    #
    # Customize your deployment here:
    #
    TRAINING_JOB_SCHEDULE: rate(24 hours) # trades off between learning delay and cost
    REWARD_ASSIGNMENT_JOB_SCHEDULE: rate(4 hours)
    TRAINING_IMAGE: 117097735164.dkr.ecr.us-west-2.amazonaws.com/improve_trainer-dev:latest 
    TRAINING_INSTANCE_TYPE: ml.m5.large
    TRAINING_INSTANCE_COUNT: 3
    TRAINING_MAX_RUNTIME_IN_SECONDS: 172800 # 2 days
    FIREHOSE_BUFFER_SIZE_IN_MB: 128 # max value is 128
    FIREHOSE_BUFFER_INTERVAL_IN_SECONDS: 900 # max value is 900
    FIREHOSE_WORKER_MEMORY_IN_MB: 1024 # must be large enough to unpack the entire firehose buffer into memory
    REWARD_ASSIGNMENT_WORKER_COUNT: 3
    DEFAULT_REWARD_WINDOW_IN_SECONDS: 172800 # 2 days
    # 
    # End customization
    #
    RECORDS_BUCKET: !Ref RecordsS3Bucket
    MODELS_BUCKET: !Ref ModelsS3Bucket
    SERVICE: ${self:service}
    STAGE: ${opt:stage, self:provider.stage}
  lambdaHashingVersion: 20201221 # can be removed in serverless v3
  name: aws
  region: us-east-1
  runtime: nodejs12.x
  timeout: 900
  memorySize: 192
  ecr:
    images:
      assign-rewards:
        path: ./src/assign_rewards/
  httpApi: # have to specify cors globally for httpApi as of 20210303
    cors: true
  iam:
    role:
      statements: 
        - Effect: Allow
          Action:
            - "firehose:*"
          Resource: { Fn::GetAtt: [Firehose, Arn] }
        - Effect: Allow
          Action:
            - "s3:*"
          Resource: 'arn:aws:s3:::${self:service}-${opt:stage, self:provider.stage}-*'
        - Effect: Allow
          Action:
            - "sagemaker:*"
          Resource: '*'
        - Effect: Allow
          Action:
            - "iam:PassRole"
          Resource: { Fn::GetAtt: [ SagemakerExecutionRole, Arn ] }
        - Effect: Allow
          Action:
            - "lambda:InvokeFunction"
            - "lambda:InvokeAsync"
          Resource: "arn:aws:lambda:${self:provider.region}:*:function:${self:service}-${opt:stage, self:provider.stage}-*"
        - Effect: Allow
          Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            - ec2:CreateNetworkInterface
            - ec2:DescribeNetworkInterfaces
            - ec2:DeleteNetworkInterface
          Resource: "*"
        - Effect: Allow
          Action:
            - elasticfilesystem:ClientMount
            - elasticfilesystem:ClientRootAccess
            - elasticfilesystem:ClientWrite
            - elasticfilesystem:DescribeMountTargets
          Resource: "*"
        - Effect: Allow
          Action:
            - batch:SubmitJob
          Resource: "arn:aws:batch:*:*:*"

service: improve-v6-${self:custom.organization}-${self:custom.project}

package:
  exclude:
    - ./**
  include:
    - 'node_modules/**'
    - 'src/**'

functions:
  track:
    description: Event Tracker HTTP API
    handler: src/track/http_api.track
    timeout: 6
    events:
      - httpApi:
          method: POST
          path: /track
    environment:
      FIREHOSE_DELIVERY_STREAM_NAME: { Ref: Firehose }

  unpackFirehose:
    description: Ingest Records from S3 to EFS
    handler: src/track/firehose.unpackFirehose
    memorySize: ${self:provider.environment.FIREHOSE_WORKER_MEMORY_IN_MB} 
    events:
      - s3:
          bucket: !Ref FirehoseS3Bucket
          existing: true # retained resource
          event: s3:ObjectCreated:*
    fileSystemConfig:
      localMountPath: /mnt/efs
      arn: { Fn::GetAtt: [ AccessPointResource, Arn ] }
    vpc:
      securityGroupIds: 
        - !Ref SecurityGroup
      subnetIds:
        - !Ref Subnet
    environment:
      EFS_FILE_PATH: /mnt/efs
        
  dispatchRewardAssignmentJobs:
    description: Dispatch Reward Assignment Batch Jobs
    runtime: python3.8
    handler: src/assign_rewards/dispatch_job.lambda_handler
    events:
      - schedule: ${self:provider.environment.REWARD_ASSIGNMENT_JOB_SCHEDULE}
    environment:
      JOB_QUEUE: !Ref JobQueue
      JOB_DEFINITION: !Ref JobDefinition

  dispatchTrainingJobs:
    description: Dispatch Decision Model Training Jobs
    handler: src/train/train.dispatchTrainingJobs
    events:
      - schedule: ${self:provider.environment.TRAINING_JOB_SCHEDULE}
    environment:
      TRAINING_ROLE_ARN: { Fn::GetAtt: [ SagemakerExecutionRole, Arn ] }

  unpackModels:
    handler: src/train/unpack_models.unpack
    events:
      - s3:
          bucket: !Ref RecordsS3Bucket
          existing: true # created in resources/serverless.yml
          event: s3:ObjectCreated:*
          rules:
            - prefix: models/
            - suffix: model.tar.gz
            
  forceDockerPush: # serverless actually wants the image to be used to deploy it
    description: Force Serverless to Push Docker Image to ECR
    image:
      name: assign-rewards

resources:
  Resources:
      FirehoseToS3Role:
        Type: AWS::IAM::Role
        Properties:
          RoleName: ${self:service}-${opt:stage, self:provider.stage}-FirehoseToS3Role
          AssumeRolePolicyDocument:
            Statement:
            - Effect: Allow
              Principal:
                Service:
                - firehose.amazonaws.com
              Action:
              - sts:AssumeRole
          Policies:
          - PolicyName: FirehoseToS3Policy
            PolicyDocument:
              Statement:
                - Effect: Allow
                  Action:
                  - s3:AbortMultipartUpload
                  - s3:GetBucketLocation
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:ListBucketMultipartUploads
                  - s3:PutObject
                  Resource: '*'
      SagemakerExecutionRole:
        Type: AWS::IAM::Role
        Properties:
          RoleName: ${self:service}-${opt:stage, self:provider.stage}-SagemakerExecutionRole
          AssumeRolePolicyDocument:
            Statement:
            - Effect: Allow
              Principal:
                Service:
                - firehose.amazonaws.com
              Action:
              - sts:AssumeRole
            - Effect: Allow
              Principal:
                Service:
                - sagemaker.amazonaws.com
              Action:
              - sts:AssumeRole
          ManagedPolicyArns:
          - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
          Policies:
          - PolicyName: SagemakerExecutionPolicy
            PolicyDocument:
              Statement:
                - Effect: Allow
                  Action:
                  - s3:GetObject
                  - s3:DeleteObject
                  - s3:ListBucket
                  - s3:PutObject
                  Resource: "arn:aws:s3:::${self:service}-${opt:stage, self:provider.stage}-**"
      FirehoseS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${opt:stage, self:provider.stage}-firehose
          IntelligentTieringConfigurations:
            - Id: '${self:service}-intelligent-tiering-firehose-${opt:stage, self:provider.stage}'
              Status: Enabled
              Tierings: # After 30 days intelligent tiering automatically moves to infrequent access tier
                - AccessTier: ARCHIVE_ACCESS
                  Days: 90
                - AccessTier: DEEP_ARCHIVE_ACCESS
                  Days: 180
      Firehose:
        Type: AWS::KinesisFirehose::DeliveryStream
        Properties:
          DeliveryStreamName: ${self:service}-${opt:stage, self:provider.stage}-firehose
          S3DestinationConfiguration:
            BucketARN:
              Fn::Join:
              - ''
              - - 'arn:aws:s3:::'
                - Ref: FirehoseS3Bucket
            BufferingHints:
              IntervalInSeconds: ${self:provider.environment.FIREHOSE_BUFFER_INTERVAL_IN_SECONDS} 
              SizeInMBs: ${self:provider.environment.FIREHOSE_BUFFER_SIZE_IN_MB} 
            CompressionFormat: "GZIP"
            RoleARN: { Fn::GetAtt: [ FirehoseToS3Role, Arn ] }
      RecordsS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${opt:stage, self:provider.stage}-records
      ModelsS3Bucket:
        Type: 'AWS::S3::Bucket'
        DeletionPolicy: Retain
        Properties:
          BucketName: ${self:service}-${opt:stage, self:provider.stage}-models
      VPC:
        Type: AWS::EC2::VPC
        Properties:
          CidrBlock: 10.0.0.0/16
          EnableDnsHostnames: true
      InternetGateway:
        Type: AWS::EC2::InternetGateway
      RouteTable:
        Type: AWS::EC2::RouteTable
        Properties:
          VpcId:
            Ref: VPC
      VPCGatewayAttachment:
        Type: AWS::EC2::VPCGatewayAttachment
        Properties:
          VpcId:
            Ref: VPC
          InternetGatewayId:
            Ref: InternetGateway
      SecurityGroup:
        Type: AWS::EC2::SecurityGroup
        Properties:
          GroupDescription: EC2 Security Group for instances launched in the VPC by Batch
          VpcId:
            Ref: VPC
          SecurityGroupIngress:
            - IpProtocol: tcp
              FromPort: 2049 #Had to open a connection for lambda to communicate with AWS EFS.
              ToPort: 2049  #In AWS internally in a security group port 2049 is stored for network file storage, 
              CidrIp: 0.0.0.0/0 #so in order to make the lambda to invoke successfully I need to add an in-bound policy or ingress policy which will allow access on that port
      Subnet:
        Type: AWS::EC2::Subnet
        Properties:
          CidrBlock: 10.0.0.0/24
          VpcId:
            Ref: VPC
          MapPublicIpOnLaunch: 'True'
      Route:
        Type: AWS::EC2::Route
        Properties:
          RouteTableId:
            Ref: RouteTable
          DestinationCidrBlock: 0.0.0.0/0
          GatewayId:
            Ref: InternetGateway
      SubnetRouteTableAssociation:
        Type: AWS::EC2::SubnetRouteTableAssociation
        Properties:
          RouteTableId:
            Ref: RouteTable
          SubnetId:
            Ref: Subnet
      S3VpcEndPoint:
        Type: 'AWS::EC2::VPCEndpoint'
        Properties:
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Principal: '*'
                Action:
                  - 's3:GetObject'
                Resource: '*'
          ServiceName: 
            Fn::Sub: 'com.amazonaws.${AWS::Region}.s3'
          VpcId: 
            Ref: VPC
          RouteTableIds:
            - Ref: RouteTable
      IamInstanceProfile:
        Type: AWS::IAM::InstanceProfile
        Properties:
          Roles:
          - Ref: EcsInstanceRole
      EcsInstanceRole:
        Type: AWS::IAM::Role
        Properties:
          AssumeRolePolicyDocument:
            Version: '2008-10-17'
            Statement:
                Effect: Allow
                Principal:
                  Service: 
                  - ec2.amazonaws.com
                Action: sts:AssumeRole
          ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role
      BatchServiceRole:
        Type: AWS::IAM::Role
        Properties:
          AssumeRolePolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Principal:
                  Service: 
                  - batch.amazonaws.com
                Action: sts:AssumeRole
          ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AWSBatchServiceRole
      AmazonEC2SpotFleetRole:
        Type: AWS::IAM::Role
        Properties:
          AssumeRolePolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Principal:
                  Service: 
                  - spotfleet.amazonaws.com
                Action: sts:AssumeRole
          ManagedPolicyArns:
            - arn:aws:iam::aws:policy/service-role/AmazonEC2SpotFleetTaggingRole
      ComputeEnvironment:
        Type: AWS::Batch::ComputeEnvironment
        Properties:
          Type: MANAGED
          ServiceRole: 
            Ref: BatchServiceRole
          ComputeEnvironmentName: '${self:service}-assign-rewards-compute-${opt:stage, self:provider.stage}'
          ComputeResources:
            MaxvCpus: 128
            SecurityGroupIds:
              - Ref: SecurityGroup
            Type: SPOT
            AllocationStrategy: SPOT_CAPACITY_OPTIMIZED
            BidPercentage: 50
            SpotIamFleetRole: 
              Ref: AmazonEC2SpotFleetRole
            # aws-ec2-spot-fleet-tagging-role
            Subnets:
              - Ref: Subnet
            MinvCpus: 0
            InstanceRole:  
              Ref: IamInstanceProfile
            InstanceTypes:
              - optimal
            DesiredvCpus: 0
            LaunchTemplate:
              LaunchTemplateId:
                Ref: LaunchTemplateResourceForBatch
              Version: 
                Fn::GetAtt: [LaunchTemplateResourceForBatch, LatestVersionNumber]
          State: ENABLED
      JobQueue:
        Type: AWS::Batch::JobQueue
        Properties:
          ComputeEnvironmentOrder:
            - Order: 1
              ComputeEnvironment: 
                Ref: ComputeEnvironment
          State: ENABLED
          Priority: 1
          JobQueueName: '${self:service}-assign-rewards-queue-${opt:stage, self:provider.stage}'
      JobDefinition:
        Type: "AWS::Batch::JobDefinition"
        Properties:
          Type: Container
          ContainerProperties: 
            Memory: 1024
            Vcpus: 1
            Image: '#{AWS::AccountId}.dkr.ecr.#{AWS::Region}.amazonaws.com/serverless-improve-v6-${opt:stage, self:provider.stage}:assign-rewards'
            MountPoints:
              - ReadOnly: false
                SourceVolume: efs
                ContainerPath: /mnt/efs
            Volumes:
              - Host:
                  SourcePath: /mnt/efs
                Name: efs
          JobDefinitionName: '${self:service}-assign-rewards-${opt:stage, self:provider.stage}'
          RetryStrategy: 
            Attempts: 2
      LaunchTemplateResourceForBatch:
        Type: AWS::EC2::LaunchTemplate
        DependsOn: FileSystemResource
        Properties:
          LaunchTemplateName: '${self:service}-launch-template-${opt:stage, self:provider.stage}'
          LaunchTemplateData:
            UserData: 
              Fn::Base64: 
                Fn::Sub:
                  - |
                    MIME-Version: 1.0
                    Content-Type: multipart/mixed; boundary="==MYBOUNDARY=="

                    --==MYBOUNDARY==
                    Content-Type: text/cloud-config; charset="us-ascii"

                    packages:
                    - amazon-efs-utils

                    - mkdir -p ${EFSDirectory}
                    - echo "${EFSFileSystem}:/ ${EFSDirectory} efs tls,_netdev" >> /etc/fstab
                    - mount -a -t efs defaults

                    --==MYBOUNDARY==--
                  - EFSFileSystem: 
                      Ref: FileSystemResource
                    EFSDirectory: /mnt/efs
      FileSystemResource:
        Type: 'AWS::EFS::FileSystem'
        Properties:
          PerformanceMode: maxIO # higher max operations per second since we do many small reads/writes
          Encrypted: true
          LifecyclePolicies:
            - TransitionToIA: AFTER_14_DAYS # when events stop arriving for a history it will transition to infrequent access
          FileSystemTags:
            - Key: Name
              Value: "${self:service}-assign-rewards-efs-${opt:stage, self:provider.stage}"
          FileSystemPolicy:
            Version: "2012-10-17"
            Statement:
              - Effect: "Allow"
                Action: "elasticfilesystem:Client*"
                Principal:
                  AWS: "*"
      MountTarget:
        Type: AWS::EFS::MountTarget
        Properties:
          FileSystemId: 
            Ref: FileSystemResource
          SecurityGroups:
            - Ref: SecurityGroup
          SubnetId: 
            Ref: Subnet
        DependsOn: Subnet
      AccessPointResource:
        Type: 'AWS::EFS::AccessPoint'
        Properties:
          FileSystemId: 
            Ref: FileSystemResource
          PosixUser:
            Uid: "1000"
            Gid: "1000"
          RootDirectory:
            CreationInfo:
              OwnerGid: "1000"
              OwnerUid: "1000"
              Permissions: "0777"
            Path: "/efs"
        DependsOn: MountTarget
